{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "import keras as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle, random, choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "\n",
    "__Input__:\n",
    "\n",
    "* Car Position\n",
    "* Car Velocity\n",
    "\n",
    "__Output__:\n",
    "\n",
    "* Choice of action [0, 1, 2]\n",
    "\n",
    "So here we're going to be using N layers:\n",
    "\n",
    "* Fully Connected 2\n",
    "* Hidden 32\n",
    "* Fully Connected 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=2),\n",
    "        Activation('relu'),\n",
    "        Dense(3),\n",
    "        Activation('softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(observation):\n",
    "    return observation.reshape(1,2)\n",
    "\n",
    "def get_action(model, state):\n",
    "    s = reshape(state)\n",
    "    result = model.predict(s)\n",
    "    return np.argmax(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function\n",
    "\n",
    "My custom reward function rewards the agent for making forward progress, without penalizing it for moving backwards, as moving backwards is necessary for gaining the required momentum to move up the hill.\n",
    "\n",
    "I unfortunately was unable to get the model to correctly learn a policy which was able to solve the problem. I think the model in my case may have been to simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, n_steps=500, epsilon=0.2):\n",
    "    experiences = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    for t in range(n_steps):\n",
    "        env.render()\n",
    "        current_state = state\n",
    "        \n",
    "        if random() > epsilon:\n",
    "            action = get_action(model, state)\n",
    "        else:\n",
    "            action = choice(range(3))\n",
    "        state, r, is_done, _ = env.step(action)\n",
    "        \n",
    "        ## Custom reward calculation\n",
    "        distance_right = state[0] + 0.5\n",
    "        distance_reward = (max(distance_right, 0) * 10) ** 2\n",
    "        custom_reward = r + distance_reward\n",
    "        \n",
    "        total_reward += custom_reward\n",
    "        exp = (current_state, action) #, custom_reward, state)\n",
    "        experiences.append(exp)\n",
    "        if is_done: \n",
    "#             print(\"Finished with Reward: {}\".format(total_reward))\n",
    "            env.close()\n",
    "            break\n",
    "    return (total_reward, experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_episodes(episodes, pct):\n",
    "    sorted_batch = sorted(episodes, key=lambda x: x[0], reverse=True)\n",
    "    top_n = int(len(sorted_batch) * pct)\n",
    "    top_episodes = sorted_batch[:top_n]\n",
    "    return top_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shuffled_xy(episodes):\n",
    "    flattened = [\n",
    "        experience\n",
    "        for episode in top_episodes\n",
    "        for experience in episode[1]\n",
    "    ]\n",
    "    \n",
    "    shuffle(flattened)\n",
    "    return get_states_actions(flattened)\n",
    "    \n",
    "def get_states_actions(episode):\n",
    "    states, actions = zip(*episode)\n",
    "    return np.array(states), to_categorical(actions, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward -188.9136845986024\n",
      "Avg Reward -189.2402057827802\n",
      "Avg Reward -189.49895020765464\n",
      "Avg Reward -189.6857238656394\n",
      "Avg Reward -189.93705159623877\n",
      "Avg Reward -189.14751161874483\n",
      "Avg Reward -187.12480402563335\n",
      "Avg Reward -188.0389604024254\n",
      "Avg Reward -187.90269768764284\n",
      "Avg Reward -187.65272190429567\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 200\n",
    "top_pct = 0.1\n",
    "n_steps = 500\n",
    "\n",
    "training_epochs = 10\n",
    "\n",
    "episode_batch = []\n",
    "model = create_model()\n",
    "for epoch in range(training_epochs):\n",
    "    for ep_n in range(n_episodes):\n",
    "        episode = run_episode(model, n_steps)\n",
    "        episode_batch.append(episode)\n",
    "    rewards = [x[0] for x in episode_batch]\n",
    "    print(\"Avg Reward\", sum(rewards) / len(rewards))\n",
    "    top_episodes = get_top_episodes(episode_batch, top_pct)\n",
    "    x_train, y_train = get_shuffled_xy(top_episodes)\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
